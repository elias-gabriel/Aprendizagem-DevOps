{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "428756b7-2ce5-4354-b945-70013d8cae8f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Python interpreter will be restarted.\n",
       "Collecting pyapacheatlas\n",
       "  Downloading pyapacheatlas-0.15.0-py3-none-any.whl (75 kB)\n",
       "Requirement already satisfied: requests&gt;=2.0 in /databricks/python3/lib/python3.8/site-packages (from pyapacheatlas) (2.25.1)\n",
       "Collecting openpyxl&gt;=3.0\n",
       "  Downloading openpyxl-3.1.2-py2.py3-none-any.whl (249 kB)\n",
       "Collecting et-xmlfile\n",
       "  Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
       "Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /databricks/python3/lib/python3.8/site-packages (from requests&gt;=2.0-&gt;pyapacheatlas) (1.25.11)\n",
       "Requirement already satisfied: certifi&gt;=2017.4.17 in /databricks/python3/lib/python3.8/site-packages (from requests&gt;=2.0-&gt;pyapacheatlas) (2020.12.5)\n",
       "Requirement already satisfied: chardet&lt;5,&gt;=3.0.2 in /databricks/python3/lib/python3.8/site-packages (from requests&gt;=2.0-&gt;pyapacheatlas) (4.0.0)\n",
       "Requirement already satisfied: idna&lt;3,&gt;=2.5 in /databricks/python3/lib/python3.8/site-packages (from requests&gt;=2.0-&gt;pyapacheatlas) (2.10)\n",
       "Installing collected packages: et-xmlfile, openpyxl, pyapacheatlas\n",
       "Successfully installed et-xmlfile-1.1.0 openpyxl-3.1.2 pyapacheatlas-0.15.0\n",
       "Python interpreter will be restarted.\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Python interpreter will be restarted.\nCollecting pyapacheatlas\n  Downloading pyapacheatlas-0.15.0-py3-none-any.whl (75 kB)\nRequirement already satisfied: requests&gt;=2.0 in /databricks/python3/lib/python3.8/site-packages (from pyapacheatlas) (2.25.1)\nCollecting openpyxl&gt;=3.0\n  Downloading openpyxl-3.1.2-py2.py3-none-any.whl (249 kB)\nCollecting et-xmlfile\n  Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\nRequirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /databricks/python3/lib/python3.8/site-packages (from requests&gt;=2.0-&gt;pyapacheatlas) (1.25.11)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /databricks/python3/lib/python3.8/site-packages (from requests&gt;=2.0-&gt;pyapacheatlas) (2020.12.5)\nRequirement already satisfied: chardet&lt;5,&gt;=3.0.2 in /databricks/python3/lib/python3.8/site-packages (from requests&gt;=2.0-&gt;pyapacheatlas) (4.0.0)\nRequirement already satisfied: idna&lt;3,&gt;=2.5 in /databricks/python3/lib/python3.8/site-packages (from requests&gt;=2.0-&gt;pyapacheatlas) (2.10)\nInstalling collected packages: et-xmlfile, openpyxl, pyapacheatlas\nSuccessfully installed et-xmlfile-1.1.0 openpyxl-3.1.2 pyapacheatlas-0.15.0\nPython interpreter will be restarted.\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pip install pyapacheatlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a809baef-9675-40d1-a317-8a8332daad6d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import col, lit\n",
    "from pyapacheatlas.auth import ServicePrincipalAuthentication\n",
    "from pyapacheatlas.core import AtlasProcess, PurviewClient, AtlasException\n",
    "from pyspark.sql.functions import col, monotonically_increasing_id, row_number\n",
    "from pyspark.sql.window import Window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17f2ab1e-e238-4229-9ba7-44cec1d97abf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tenant_id = \"\"\n",
    "client_id = \"\"\n",
    "client_secret = \"\"\n",
    "data_catalog_name = \"\"\n",
    "resource_url = 'https://purview.azure.net'\n",
    "guid_start = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f266afb5-4bd2-477f-b527-4d6ef5651964",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def azuread_auth(tenant_id: str, client_id: str, client_secret: str, resource_url: str):\n",
    "    \"\"\"\n",
    "    Authenticates Service Principal to the provided Resource URL, and returns the OAuth Access Token\n",
    "    \"\"\"\n",
    "    url = f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\"\n",
    "    payload= f'grant_type=client_credentials&client_id={client_id}&client_secret={client_secret}&resource={resource_url}'\n",
    "    headers = {\n",
    "    'Content-Type': 'application/x-www-form-urlencoded'\n",
    "    }\n",
    "    response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "    access_token = json.loads(response.text)['access_token']\n",
    "    return access_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c7fe5b5-d16e-404a-ae91-a1bfe81e1d90",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_purview_assets(data_catalog_name: str, azuread_access_token: str, table_name: str):\n",
    "    url = f\"https://{data_catalog_name}.purview.azure.com/catalog/api/search/query?api-version=2022-08-01-preview\"\n",
    "\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {azuread_access_token}',\n",
    "        'Content-Type': 'application/json'\n",
    "        }\n",
    "\n",
    "    payload = \"\"\"{\n",
    "        \"keywords\": null,\n",
    "        \"limit\": 1000,\n",
    "        \"filter\": {\n",
    "            \"and\": [\n",
    "            {\n",
    "                \"attributeName\": \"qualifiedName\",\n",
    "                \"operator\": \"contains\",\n",
    "                \"attributeValue\": \"%s\"\n",
    "            }\n",
    "            ]\n",
    "        }\n",
    "    }\"\"\" % (table_name)\n",
    "\n",
    "    response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "    data = response.json()\n",
    "\n",
    "    # Regex pattern to match the desired qualifiedName format\n",
    "    pattern = re.compile(r'^.+@https://.+\\.azuredatabricks\\.net/\\?o$')\n",
    "\n",
    "    # Filter the results to find the entity with the desired qualifiedName format only for hive_table entityType\n",
    "    filtered_data = [item for item in data['value'] if item['entityType'] != 'hive_table' or pattern.match(item['qualifiedName'])]\n",
    "\n",
    "    return filtered_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\"\"\"\n",
    "\n",
    "SELECT nomeAmbiente, tipoOrigem, nomeCamada, nomeOrigem, nomeTabelaOrigem, nomeArquivoOrigem, nomeDatabaseDatabricks, nomeTabelaDatabricks\n",
    "FROM 0_par.processos \n",
    "WHERE nomeTabelaOrigem IS NOT NULL \n",
    "AND nomeTabelaOrigem <> ''\n",
    "AND nomeTabelaOrigem NOT LIKE '%teste%'\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "schema = \"guid_origem STRING, qualified_name_origem STRING, name_origem STRING, entityType_origem STRING, guid_destino STRING, qualified_name_destino STRING, name_destino STRING, entityType_destino STRING\"\n",
    "all_rows = []\n",
    "\n",
    "for row in df.collect():\n",
    "    origem_table = row['nomeTabelaOrigem']\n",
    "    origem_file = row['nomeArquivoOrigem']\n",
    "    destino_table = f\"{row['nomeDatabaseDatabricks']}.{row['nomeTabelaDatabricks']}\"\n",
    "\n",
    "    try:\n",
    "        azuread_access_token = azuread_auth(tenant_id, client_id, client_secret, resource_url)\n",
    "        origem_data = get_purview_assets(data_catalog_name, azuread_access_token, origem_table)\n",
    "\n",
    "        if not origem_data:  \n",
    "            origem_data = get_purview_assets(data_catalog_name, azuread_access_token, origem_file)\n",
    "            if not origem_data:\n",
    "                print(f\"No results found for source: {origem_file}\")\n",
    "\n",
    "        if origem_data:\n",
    "            origem_item = origem_data[0]\n",
    "            origem_values = (origem_item['id'], origem_item['qualifiedName'].split(\"@\")[0], origem_item['name'], origem_item['entityType'])\n",
    "        else:\n",
    "            origem_values = (None, None, None, None)\n",
    "\n",
    "        azuread_access_token = azuread_auth(tenant_id, client_id, client_secret, resource_url)\n",
    "        destino_data = get_purview_assets(data_catalog_name, azuread_access_token, destino_table)\n",
    "\n",
    "        if destino_data:\n",
    "            destino_item = destino_data[0]\n",
    "            destino_values = (destino_item['id'], destino_item['qualifiedName'].split(\"@\")[0], destino_item['name'], destino_item['entityType'])\n",
    "        else:\n",
    "            destino_values = (None, None, None, None)\n",
    "            print(f\"No results found for destination: {destino_table}\")\n",
    "\n",
    "        row_values = origem_values + destino_values\n",
    "        all_rows.append(Row(*row_values))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error while processing tables: {origem_table}, {destino_table}. Error: {e}\")\n",
    "\n",
    "final_df = spark.createDataFrame(all_rows, schema)\n",
    "display(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_condition = col('guid_origem').isNotNull()\n",
    "\n",
    "other_columns = [\n",
    "    'qualified_name_origem',\n",
    "    'name_origem',\n",
    "    'entityType_origem',\n",
    "    'guid_destino',\n",
    "    'qualified_name_destino',\n",
    "    'name_destino',\n",
    "    'entityType_destino'\n",
    "]\n",
    "\n",
    "distinct_final_df = final_df.filter(filter_condition)\n",
    "distinct_final_df = distinct_final_df.withColumn('id', monotonically_increasing_id())\n",
    "distinct_final_df = distinct_final_df.withColumn('id', row_number().over(Window.orderBy('id')))\n",
    "distinct_final_df = distinct_final_df.distinct()\n",
    "distinct_final_tb = distinct_final_df.select('id', 'guid_origem', 'qualified_name_origem','name_origem','entityType_origem','guid_destino','qualified_name_destino','name_destino','entityType_destino')\n",
    "\n",
    "distinct_final_tb.write.format(\"delta\").mode('overwrite').saveAsTable(\"0_par.lineage_data\")\n",
    "\n",
    "distinct_final_df = distinct_final_df.withColumn('action', lit(0))\n",
    "distinct_final_df = distinct_final_df.select(col('id'), 'guid_origem', 'guid_destino', 'action')\n",
    "\n",
    "#distinct_final_df.write.format(\"delta\").mode('overwrite').saveAsTable(\"0_par.lineage_actions\")\n",
    "\n",
    "display(distinct_final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_final_df.createOrReplaceTempView(\"distinct_final_df_temp_view\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "MERGE INTO 0_par.lineage_actions AS target\n",
    "USING distinct_final_df_temp_view AS source\n",
    "ON target.guid_origem = source.guid_origem AND target.guid_destino = source.guid_destino\n",
    "WHEN NOT MATCHED THEN\n",
    "  INSERT (id, guid_origem, guid_destino, action)\n",
    "  VALUES (source.id, source.guid_origem, source.guid_destino, source.action)\n",
    "\"\"\")\n",
    "\n",
    "spark.sql('SELECT * FROM 0_par.lineage_actions').display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "305a4a38-2a49-4b24-8973-605b54af445b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_lineage(source_guid, destination_guid):\n",
    "    global guid_start\n",
    "    \n",
    "    result = spark.sql(f\"SELECT id, action FROM 0_par.lineage_actions WHERE guid_origem = '{source_guid}' AND guid_destino = '{destination_guid}'\").collect()\n",
    "\n",
    "    if result:\n",
    "        row_start = result[0][0]\n",
    "        if result[0][1] == 1:\n",
    "            return f\"Lineage already exists for source_guid: {source_guid} and destination_guid: {destination_guid}, ID: {row_start}\\n\"\n",
    "            \n",
    "    auth = ServicePrincipalAuthentication(\n",
    "        tenant_id = \"\",\n",
    "        client_id = \"\",\n",
    "        client_secret = \"\"\n",
    "    )\n",
    "\n",
    "    client = PurviewClient(\n",
    "        account_name=\"\",\n",
    "        authentication=auth\n",
    "    )\n",
    "    \n",
    "    process_qn = f'Notebook: Purview - Create Lineage {row_start}'\n",
    "    process_type_name = 'Process'\n",
    "\n",
    "    new_lineage = AtlasProcess(\n",
    "        name= f'Notebook Processing {row_start}',\n",
    "        typeName=process_type_name,\n",
    "        qualified_name=process_qn,\n",
    "        inputs=[{\"guid\": source_guid}],\n",
    "        outputs=[{\"guid\": destination_guid}],\n",
    "        guid=guid_start\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        results = client.upload_entities(batch=[new_lineage])\n",
    "        spark.sql(f\"UPDATE 0_par.lineage_actions SET action = 1 WHERE guid_origem = '{source_guid}' AND guid_destino = '{destination_guid}'\")\n",
    "        guid_start -= 1\n",
    "        print(f\"Lineage created for source_guid: {source_guid} and destination_guid: {destination_guid}\\n\")\n",
    "    except AtlasException as e:\n",
    "            return f\"Failed to create lineage for source_guid: {source_guid} and destination_guid: {destination_guid}. Error message: {e}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f = spark.sql(\"SELECT * FROM 0_par.lineage_actions\")\n",
    "\n",
    "for index, row in enumerate(df_f.collect()):\n",
    "    print(f\"Processing row {index+1}: {row}\")\n",
    "    source_guid = row['guid_origem']\n",
    "    destination_guid = row['guid_destino']\n",
    "\n",
    "    lineage_result = create_lineage(source_guid, destination_guid)\n",
    "    print(lineage_result)\n",
    "\n",
    "    time.sleep(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_entity = ''\n",
    "destination_entity = ''\n",
    "\n",
    "auth = ServicePrincipalAuthentication(\n",
    "        tenant_id = \"\",\n",
    "        client_id = \"\",\n",
    "        client_secret = \"\"\n",
    "    )\n",
    "\n",
    "client = PurviewClient(\n",
    "        account_name=\"\",\n",
    "        authentication=auth\n",
    "    )\n",
    "\n",
    "\n",
    "process_type_name = 'Process'\n",
    "process_qn = f'Notebook: Purview - Create Lineage'\n",
    "\n",
    "new_lineage = AtlasProcess(\n",
    "            name= f'Notebook Processing',\n",
    "            typeName=process_type_name,\n",
    "            qualified_name=process_qn,\n",
    "            inputs=[{\"guid\": source_entity}],\n",
    "            outputs=[{\"guid\": destination_entity}],\n",
    "            guid=-1\n",
    "        )\n",
    "\n",
    "results = client.upload_entities(batch=[new_lineage])\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3538096544798351,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "purview_mn",
   "notebookOrigID": 2925738933848097,
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
